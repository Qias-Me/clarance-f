/**
 * Fix Section 30 Field Duplicates Script
 * 
 * This script removes duplicate field entries from section-30.json that are
 * causing field mapping collisions in the PDF service.
 */

const fs = require('fs');
const path = require('path');

console.log('ğŸ”§ ===== FIXING SECTION 30 FIELD DUPLICATES =====\n');

// ============================================================================
// LOAD SECTION 30 REFERENCE DATA
// ============================================================================

const sectionFilePath = path.join(__dirname, 'api', 'sections-references', 'section-30.json');
console.log(`ğŸ“‚ Loading section data from: ${sectionFilePath}`);

if (!fs.existsSync(sectionFilePath)) {
  console.error(`âŒ File not found: ${sectionFilePath}`);
  process.exit(1);
}

const sectionData = JSON.parse(fs.readFileSync(sectionFilePath, 'utf8'));
console.log(`âœ… Loaded section data with ${sectionData.fields.length} fields`);

// ============================================================================
// ANALYZE DUPLICATES
// ============================================================================

console.log('\nğŸ” ===== ANALYZING FIELD DUPLICATES =====\n');

const fieldIdCounts = new Map();
const duplicateFields = new Map();

// Count occurrences of each field ID
sectionData.fields.forEach((field, index) => {
  const cleanId = field.id.replace(' 0 R', '');
  
  if (!fieldIdCounts.has(cleanId)) {
    fieldIdCounts.set(cleanId, []);
  }
  fieldIdCounts.get(cleanId).push({ field, index });
});

// Identify duplicates
fieldIdCounts.forEach((occurrences, fieldId) => {
  if (occurrences.length > 1) {
    duplicateFields.set(fieldId, occurrences);
    console.log(`ğŸ”„ Field ID ${fieldId} appears ${occurrences.length} times:`);
    occurrences.forEach((occurrence, i) => {
      console.log(`  [${i+1}] Index ${occurrence.index}: ${occurrence.field.name} (${occurrence.field.label})`);
    });
    console.log('');
  }
});

console.log(`ğŸ“Š Total unique field IDs: ${fieldIdCounts.size}`);
console.log(`ğŸ”„ Duplicate field IDs: ${duplicateFields.size}`);

// ============================================================================
// REMOVE DUPLICATES
// ============================================================================

console.log('\nğŸ§¹ ===== REMOVING DUPLICATE FIELDS =====\n');

const fieldsToRemove = [];
const keptFields = new Set();

duplicateFields.forEach((occurrences, fieldId) => {
  console.log(`ğŸ”§ Processing duplicates for field ${fieldId}:`);
  
  // Keep the first occurrence, mark others for removal
  const firstOccurrence = occurrences[0];
  keptFields.add(firstOccurrence.index);
  console.log(`  âœ… Keeping occurrence at index ${firstOccurrence.index}`);
  
  // Mark other occurrences for removal
  for (let i = 1; i < occurrences.length; i++) {
    const occurrence = occurrences[i];
    fieldsToRemove.push(occurrence.index);
    console.log(`  âŒ Removing occurrence at index ${occurrence.index}`);
  }
  console.log('');
});

console.log(`ğŸ“Š Fields to remove: ${fieldsToRemove.length}`);
console.log(`ğŸ“Š Fields to keep: ${sectionData.fields.length - fieldsToRemove.length}`);

// ============================================================================
// CREATE CLEANED DATA
// ============================================================================

console.log('\nğŸ—ï¸ ===== CREATING CLEANED DATA =====\n');

// Sort removal indices in descending order to avoid index shifting issues
fieldsToRemove.sort((a, b) => b - a);

const originalFieldCount = sectionData.fields.length;
const cleanedFields = [...sectionData.fields];

fieldsToRemove.forEach(index => {
  console.log(`ğŸ—‘ï¸ Removing field at index ${index}: ${cleanedFields[index].name}`);
  cleanedFields.splice(index, 1);
});

// Update the section data
const cleanedSectionData = {
  ...sectionData,
  fields: cleanedFields,
  metadata: {
    ...sectionData.metadata,
    totalFields: cleanedFields.length
  }
};

console.log(`âœ… Cleaned data created:`);
console.log(`   ğŸ“Š Original field count: ${originalFieldCount}`);
console.log(`   ğŸ“Š Cleaned field count: ${cleanedFields.length}`);
console.log(`   ğŸ“Š Fields removed: ${originalFieldCount - cleanedFields.length}`);

// ============================================================================
// VERIFY NO DUPLICATES REMAIN
// ============================================================================

console.log('\nğŸ” ===== VERIFYING CLEANED DATA =====\n');

const verificationCounts = new Map();
cleanedFields.forEach(field => {
  const cleanId = field.id.replace(' 0 R', '');
  verificationCounts.set(cleanId, (verificationCounts.get(cleanId) || 0) + 1);
});

let hasDuplicates = false;
verificationCounts.forEach((count, fieldId) => {
  if (count > 1) {
    console.log(`âŒ Still has duplicates: Field ${fieldId} appears ${count} times`);
    hasDuplicates = true;
  }
});

if (!hasDuplicates) {
  console.log(`âœ… Verification passed: No duplicate field IDs remain`);
  console.log(`ğŸ“Š Unique field IDs: ${verificationCounts.size}`);
} else {
  console.error(`âŒ Verification failed: Duplicates still exist`);
  process.exit(1);
}

// ============================================================================
// SAVE CLEANED DATA
// ============================================================================

console.log('\nğŸ’¾ ===== SAVING CLEANED DATA =====\n');

// Create backup of original file
const backupPath = sectionFilePath + '.backup';
console.log(`ğŸ“‹ Creating backup: ${backupPath}`);
fs.writeFileSync(backupPath, JSON.stringify(sectionData, null, 2));

// Save cleaned data
console.log(`ğŸ’¾ Saving cleaned data: ${sectionFilePath}`);
fs.writeFileSync(sectionFilePath, JSON.stringify(cleanedSectionData, null, 2));

console.log('âœ… File saved successfully');

// ============================================================================
// SPECIFIC FIELD 16262 VALIDATION
// ============================================================================

console.log('\nğŸ¯ ===== FIELD 16262 VALIDATION =====\n');

const field16262 = cleanedFields.find(f => f.id.replace(' 0 R', '') === '16262');
if (field16262) {
  console.log(`âœ… Field 16262 found and unique:`);
  console.log(`   ğŸ†” ID: ${field16262.id}`);
  console.log(`   ğŸ“› Name: ${field16262.name}`);
  console.log(`   ğŸ·ï¸ Label: ${field16262.label}`);
  console.log(`   ğŸ“ Type: ${field16262.type}`);
  console.log(`   ğŸ“ Max Length: ${field16262.maxLength}`);
  console.log(`   ğŸ”— Unique ID: ${field16262.uniqueId}`);
} else {
  console.error(`âŒ Field 16262 not found after cleanup!`);
  process.exit(1);
}

console.log('\nğŸ‰ ===== FIX COMPLETED SUCCESSFULLY =====\n');
console.log('âœ… Section 30 field duplicates have been removed');
console.log('âœ… Field 16262 is now unique and should not cause mapping collisions');
console.log('âœ… Backup created in case rollback is needed');
console.log('\nNext steps:');
console.log('1. Test the PDF generation with Section 30 data');
console.log('2. Verify that date values go to correct date fields');
console.log('3. Verify that ZIP code values go to the ZIP code field');
console.log('4. If successful, the backup file can be deleted'); 